{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56c208d7",
   "metadata": {},
   "source": [
    "This section shows the process of getting the data from Billboards WebSite and Twitter using `selenium` web drive the `snscrape` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "900e69c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to import some packages. If you do not have intelled a specific package, you can install it using either - conda install or pip install.\n",
    "import requests # to gather online data using methods such as GET and POST\n",
    "import time # time modules \n",
    "import os #operational system \n",
    "from bs4 import BeautifulSoup # html interp\n",
    "from selenium import webdriver #browser simulator\n",
    "from selenium.webdriver.chrome.options import Options \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import glob\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "import snscrape.modules.twitter as sntwitter #twitter scrapper library\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer #sentiment analyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f213968b",
   "metadata": {},
   "source": [
    "The billboard website is written in Javascript and doesn't have an open API. These website features prevent us from using the library Requests directly to get the information wanted. To overcome this issue we need to set up a WebDrive browser simulator. I'm using a chrome simulator and you can download the web driver [here](https://chromedriver.chromium.org/downloads). With the code below in hand, you can set up a chrome simulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb881a5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.chrome.webdriver.WebDriver (session=\"d7cadb81dab30f59aeb195f13d6d2760\")>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('C:/Users/joaom/Desktop') # change  directory\n",
    "chrome_driver = 'C://Users//joaom//Desktop//chromedriver' #set the webdriver \n",
    "#Set up chromedrive options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--window-size=1366x768\")\n",
    "#set the browser simulator\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "driver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579ba79e",
   "metadata": {},
   "source": [
    "If you want to save this data straight to your `SQL` server you can run this code below with your `SQL` server user and password."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d08bd783",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(\"mysql://User:password@localhost/database\")\n",
    "con = engine.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db8b009",
   "metadata": {},
   "source": [
    "In order to save the data in a `SQL` server, we need to treat the strings first because there are some symbols that are not allowed in the `SQL` language. To do this, I created two functions: the first one is to strip accents of the text using `unicodedata`, and the second is where we use `Regular Expressions` to substitute the not allowed symbols in `SQL`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2193fb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "## These functions are important to strip accents and other special characteres from our data \n",
    "#before inserting them into MySQL \n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def strip_accents(text):\n",
    "    \"\"\"\n",
    "    Strip accents from input String.\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        text = unicode(text, 'utf-8')\n",
    "    except (TypeError, NameError): # unicode is a default on python 3 \n",
    "        pass\n",
    "    text = unicodedata.normalize('NFD', text)\n",
    "    text = text.encode('ascii', 'ignore')\n",
    "    text = text.decode(\"utf-8\")\n",
    "    return str(text)\n",
    "\n",
    "def text_to_sql(text):\n",
    "    \"\"\"\n",
    "    Convert input text to id.\n",
    "    \"\"\"\n",
    "    text = strip_accents(text.lower())\n",
    "    text = re.sub('[^@$!?&.#0-9a-zA-Z_-]',\" \", text)\n",
    "    text= text.lstrip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2c6951",
   "metadata": {},
   "source": [
    "After investigating the Billboards website HTML code look for the charts table, we were able to create the web crawler. With the following function, we are able to get the charts of any given week on the Hot 100 historical charts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25de8cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_charts(weekdate):\n",
    "    url= \"https://www.billboard.com/charts/hot-100\" +'/'+weekdate\n",
    "    driver.get(url)\n",
    "    html_source = driver.page_source\n",
    "    soup = BeautifulSoup(html_source, 'lxml')\n",
    "    data=[]\n",
    "    weekdate=weekdate\n",
    "    for music in range(100):\n",
    "        rawdata= soup.find_all('div', attrs={'class' : 'o-chart-results-list-row-container'})[music].text.replace('NEW','').replace('ENTRY','').replace('RE-','').splitlines()\n",
    "        fields={}\n",
    "        x=[]\n",
    "        for i in rawdata:    \n",
    "            if i != '':\n",
    "                text= strip_accents(i)\n",
    "                text=text_to_sql(text)\n",
    "                x.append(text) \n",
    "        fields['position']=x[0]\n",
    "        fields['music']=x[1]\n",
    "        fields['artist']=x[2]\n",
    "        fields['lastweek']=x[3]\n",
    "        fields['peak'] = x[4]\n",
    "        fields['weeks'] = x[5]\n",
    "        fields['weekdate']=weekdate\n",
    "        data.append(fields)\n",
    "    data = pd.DataFrame(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8336f48",
   "metadata": {},
   "source": [
    "But the weekly charts are not the only data we need, we have to get the tweets that contain the songs on the chart and the information in these tweets, as the tweet content and the favorites and retweets numbers. So with the following function, we are able to get the data from billboard and Twitter merged in one dataset just inserting a date. Notice the tweets are collected from one week lag to the inserted week date in the YY-MM-DD format. I'm doing this because the alleged tweets' impact on Billboard charts is counted before the weekly charts are updated. The function uses `snscrapper` to overcome the Twitter's API limits. The data can be stored in a SQL server or save as Pickle or any other data file supported by `Pandas`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f54fbf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weekly_data(weekdate):\n",
    "    charts = get_charts(weekdate)\n",
    "    weekdate=pd.to_datetime(weekdate)\n",
    "    def get_tweets(charts, weekdate):\n",
    "        data = charts.assign(search= lambda x: x['artist'] + ' AND ' + x['music'] )\n",
    "        tweets_list = []\n",
    "        for search in data['search']:\n",
    "            music= search.split('AND ')[-1]\n",
    "            until = weekdate.strftime('%Y-%m-%d')\n",
    "            since = (weekdate - pd.to_timedelta(1, 'W')).strftime('%Y-%m-%d')\n",
    "            query = search +  ' lang:en since:{} until:{}'.format(since, until)\n",
    "            for tweet in itertools.islice(sntwitter.TwitterSearchScraper(query).get_items(), 0,100,None):\n",
    "                fields = {}\n",
    "                date = str(tweet.date)     \n",
    "                text = str(tweet.content)\n",
    "                text = strip_accents(text)\n",
    "                text = text_to_sql(text)\n",
    "                username = str(tweet.user.username)\n",
    "                favorites = str(tweet.likeCount)\n",
    "                retweets = str(tweet.retweetCount)\n",
    "                fields['datetime'] = date\n",
    "                fields['usarname'] = username\n",
    "                fields['text'] = text\n",
    "                fields['favorites'] = favorites\n",
    "                fields['retweets'] = retweets\n",
    "                fields['music'] = music\n",
    "                tweets_list.append(fields)\n",
    "        tweets_df = pd.DataFrame(tweets_list)\n",
    "        return tweets_df\n",
    "    tweets = get_tweets(charts, weekdate)\n",
    "    week_data = charts.merge(tweets,\n",
    "                             left_on=\"music\",\n",
    "                             right_on=\"music\",\n",
    "                             how=\"outer\",\n",
    "                             indicator=True)\n",
    "    week_data = week_data.drop('_merge', axis=1)\n",
    "    week_data.to_pickle('C:/Users/joaom/Desktop/data/data_'+\n",
    "                        weekdate.strftime('%Y')+'/data_'+\n",
    "                        ''.join(weekdate.strftime('%Y-%m-%d').split('-'))+'.pkl')\n",
    "    #week_data.to_sql(name= 'data_'+''.join(weekdate.strftime('%Y-%m-%d').split('-')),con=con,if_exists='replace')\n",
    "    return week_data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c509f5",
   "metadata": {},
   "source": [
    "For instance, we can get the data for my 2020's birthday.\n",
    "\n",
    "It's note worthing knowing that this process is not the most fast method to web scrap, so It will take couple of minutes to gathered the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09f9dbd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>position</th>\n",
       "      <th>music</th>\n",
       "      <th>artist</th>\n",
       "      <th>lastweek</th>\n",
       "      <th>peak</th>\n",
       "      <th>weeks</th>\n",
       "      <th>weekdate</th>\n",
       "      <th>datetime</th>\n",
       "      <th>usarname</th>\n",
       "      <th>text</th>\n",
       "      <th>favorites</th>\n",
       "      <th>retweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>the scotts</td>\n",
       "      <td>the scotts  travis scott &amp; kid cudi</td>\n",
       "      <td>-</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-05-09</td>\n",
       "      <td>2020-05-08 23:40:55+00:00</td>\n",
       "      <td>Steven_Patz</td>\n",
       "      <td>the scotts by the scotts  travis scott  kid cu...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>the scotts</td>\n",
       "      <td>the scotts  travis scott &amp; kid cudi</td>\n",
       "      <td>-</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-05-09</td>\n",
       "      <td>2020-05-08 22:16:35+00:00</td>\n",
       "      <td>MusikhedRadio</td>\n",
       "      <td>travis scott  kid cudi - the scotts</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>the scotts</td>\n",
       "      <td>the scotts  travis scott &amp; kid cudi</td>\n",
       "      <td>-</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-05-09</td>\n",
       "      <td>2020-05-08 20:37:27+00:00</td>\n",
       "      <td>BarzFan</td>\n",
       "      <td>the scotts a song by kid cudi  the scotts  and...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>the scotts</td>\n",
       "      <td>the scotts  travis scott &amp; kid cudi</td>\n",
       "      <td>-</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-05-09</td>\n",
       "      <td>2020-05-08 20:33:26+00:00</td>\n",
       "      <td>Ayarrrod99</td>\n",
       "      <td>the scotts rmx out now someone let travis scot...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>the scotts</td>\n",
       "      <td>the scotts  travis scott &amp; kid cudi</td>\n",
       "      <td>-</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-05-09</td>\n",
       "      <td>2020-05-08 20:20:11+00:00</td>\n",
       "      <td>squatterant</td>\n",
       "      <td>travis scott debuts kid cudi collab  the scott...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  position       music                               artist lastweek peak  \\\n",
       "0        1  the scotts  the scotts  travis scott & kid cudi        -    1   \n",
       "1        1  the scotts  the scotts  travis scott & kid cudi        -    1   \n",
       "2        1  the scotts  the scotts  travis scott & kid cudi        -    1   \n",
       "3        1  the scotts  the scotts  travis scott & kid cudi        -    1   \n",
       "4        1  the scotts  the scotts  travis scott & kid cudi        -    1   \n",
       "\n",
       "  weeks    weekdate                   datetime       usarname  \\\n",
       "0     1  2020-05-09  2020-05-08 23:40:55+00:00    Steven_Patz   \n",
       "1     1  2020-05-09  2020-05-08 22:16:35+00:00  MusikhedRadio   \n",
       "2     1  2020-05-09  2020-05-08 20:37:27+00:00        BarzFan   \n",
       "3     1  2020-05-09  2020-05-08 20:33:26+00:00     Ayarrrod99   \n",
       "4     1  2020-05-09  2020-05-08 20:20:11+00:00    squatterant   \n",
       "\n",
       "                                                text favorites retweets  \n",
       "0  the scotts by the scotts  travis scott  kid cu...         0        0  \n",
       "1                travis scott  kid cudi - the scotts         0        0  \n",
       "2  the scotts a song by kid cudi  the scotts  and...         1        1  \n",
       "3  the scotts rmx out now someone let travis scot...         0        0  \n",
       "4  travis scott debuts kid cudi collab  the scott...         0        0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Birthday_data = get_weekly_data('2020-05-09')\n",
    "birthday_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758a2d76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "706ce8e4",
   "metadata": {},
   "source": [
    "Now we need to automatize the process of inserting in the fuction the date in the right format. To do this, I created a function where you insert a year the funtion will return the weekly data of a given year. I used `pandas.Datetime` features and a simple `for loop`. As the Billboard Hot100 is updated every Thursday, the week inserted in the `get_weekly_data()` will be all the thursday of a given year. As twitter was open in 2007, I decided to get the data from 2008 and on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66060273",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_data(year):\n",
    "    def all_thursday(year):\n",
    "        return pd.date_range(start=str(year),\n",
    "                             end=str(year+1),\n",
    "                             freq='W-Thu').strftime('%Y-%m-%d').tolist()\n",
    "    thursdays = all_thursday(year)\n",
    "    for thursday in thursdays:\n",
    "        get_weekly_data(thursday)\n",
    "for i in range(2008,2022):\n",
    "    all_data(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8d97aa",
   "metadata": {},
   "source": [
    "Now with the data in hand, we can go for the part two of this project, the data treatment and vizualization and some insights. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
